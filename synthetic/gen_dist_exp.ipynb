{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0903538f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Unifying distillation and privileged information - Synthetic experiment 1 \n",
    "This notebook reproduces the synthetic experiments 1 and 3 from [Unifying Distillation and Privileged Information by Lopez et al.](https://arxiv.org/abs/1511.03643).\n",
    "\n",
    "This notebooks demonstrates: 1) That the teacher can be replaced with a Cumulative Distribution Function of our privileged information $z$, and 2) that the effects of Generalised Distillation are very limited in terms of sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd62644",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc5a80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Synthetic environments\n",
    "\n",
    "1. **Clean labels as privileged information.** We sample triplets $(x_i, x^\\star_i, y_i)$ from:\n",
    "\n",
    "\\begin{align*}\n",
    "  x_i       &\\sim \\mathcal{N}(0,I_d)\\\\\n",
    "  x^\\star_i &\\leftarrow \\langle \\alpha, x_i \\rangle\\\\\n",
    "  \\varepsilon_i &\\sim \\mathcal{N}(0,1)\\\\\n",
    "  y_i       &\\leftarrow \\mathbb{I}((x^\\star_i + \\varepsilon_i) > 0).\n",
    "\\end{align*}\n",
    "\n",
    "3. **Experiment 3: Relevant features as PI** We sample triplets $(x_i, x^\\star_i, y_i)$ from:\n",
    "\\begin{align*}\n",
    "    & x_i \\sim \\mathcal{N}(0, I_d)\\\\\n",
    "    & z_i \\leftarrow x_{i, J} \\\\\n",
    "    \\\\\n",
    "    & y_i \\leftarrow \\mathbb{I} \\left \\{ \\langle \\alpha, z_i \\rangle > 0 \\right \\}, \\\\\n",
    "\\end{align*}\n",
    "\n",
    "For a more detailed explanation of the synthetic environments, we refer to the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef14111e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# experiment 1: noiseless labels as privileged info\n",
    "def synthetic_01(a,n):\n",
    "    x  = np.random.randn(n,a.size)\n",
    "    e  = (np.random.randn(n))[:,np.newaxis]\n",
    "    xs = np.dot(x,a)[:,np.newaxis]\n",
    "    y  = ((xs+e) > 0).ravel()\n",
    "    return (xs,x,y)\n",
    "\n",
    "# experiment 3: relevant inputs as privileged info\n",
    "def synthetic_03(a,n):\n",
    "    x  = np.random.randn(n,a.size)\n",
    "    xs = np.copy(x)\n",
    "    xs = xs[:,0:3]\n",
    "    a  = a[0:3]\n",
    "    y  = (np.dot(xs,a) > 0).ravel()\n",
    "    return (xs,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f1d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fb7120",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fitModel(model,optimizer,criterion,epochs,x,target, linear=False):\n",
    "    for epoch in range(epochs):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "        if not linear:\n",
    "            y_pred,_ = model(x)\n",
    "        else:\n",
    "            y_pred = model(x)\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, target)\n",
    "        #print(epoch, loss.data[0])\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c1a35f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(w, t = 1.0):\n",
    "    e = np.exp(w / t)\n",
    "    return e/np.sum(e,1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f40c54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,d,q):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d, q),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc(x)\n",
    "        x1 = F.softmax(x, dim=1)\n",
    "        return x1,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a1ef4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_exp(x_tr,xs_tr,y_tr,x_te,xs_te,y_te):\n",
    "    t = 1\n",
    "    l = 1\n",
    "    l_r=0.001\n",
    "    epochs=1000\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # scale stuff\n",
    "    s_x   = StandardScaler().fit(x_tr)\n",
    "    s_s   = StandardScaler().fit(xs_tr)\n",
    "    x_tr  = s_x.transform(x_tr)\n",
    "    x_te  = s_x.transform(x_te)\n",
    "    xs_tr = s_s.transform(xs_tr)\n",
    "    xs_te = s_s.transform(xs_te)\n",
    "    y_tr  = y_tr*1.0\n",
    "    y_te  = y_te*1.0\n",
    "    y_tr  = np.vstack((y_tr==1,y_tr==0)).T\n",
    "    z_tr  = norm.cdf(xs_tr)\n",
    "    z_tr  = np.vstack((z_tr.T,1-z_tr.T)).T\n",
    "    y_te  = np.vstack((y_te==1,y_te==0)).T\n",
    "    \"\"\"\n",
    "    Training of privileged model\n",
    "    \"\"\"\n",
    "    xs_tr = Variable(torch.from_numpy(xs_tr)).type(torch.FloatTensor)\n",
    "    y_tr = Variable(torch.from_numpy(y_tr*1.0)).type(torch.FloatTensor)\n",
    "    mlp_priv = Net(xs_tr.shape[1],2)\n",
    "    optimizer = optim.RMSprop(mlp_priv.parameters(),lr=l_r)\n",
    "    mlp_priv=fitModel(mlp_priv,optimizer,criterion,epochs,xs_tr,y_tr)    \n",
    "    xs_te = Variable(torch.from_numpy(xs_te)).type(torch.FloatTensor)\n",
    "    _,soften=mlp_priv(xs_tr)\n",
    "    output,_=mlp_priv(xs_te)\n",
    "    pred = torch.argmax(output,dim=1)\n",
    "    pred=pred.numpy()\n",
    "    res_priv=np.mean(pred==np.argmax(y_te,1))\n",
    "    \"\"\"\n",
    "    Training of regular MLP\n",
    "    \"\"\"\n",
    "    x_tr = Variable(torch.from_numpy(x_tr)).type(torch.FloatTensor)\n",
    "    mlp_reg = Net(x_tr.shape[1],2)\n",
    "    optimizer = optim.RMSprop(mlp_reg.parameters(),lr=l_r)\n",
    "    mlp_reg=fitModel(mlp_reg,optimizer,criterion,epochs,x_tr,y_tr)\n",
    "    x_te = Variable(torch.from_numpy(x_te)).type(torch.FloatTensor)\n",
    "    output,_=mlp_reg(x_te)\n",
    "    pred = torch.argmax(output,dim=1)\n",
    "    pred=pred.numpy()\n",
    "    res_reg=np.mean(pred==np.argmax(y_te,1))\n",
    "\n",
    "    softened=soften.detach()\n",
    "    softened=softened.numpy()\n",
    "    p_tr=softmax(softened,t)\n",
    "    p_tr=Variable(torch.from_numpy(p_tr)).type(torch.FloatTensor)\n",
    "    \n",
    "    ### freezing layers\n",
    "    for param in mlp_priv.parameters():\n",
    "        param.requires_grad =False\n",
    "    \"\"\"\n",
    "    LUPI Combination of two model\n",
    "    \"\"\"\n",
    "    mlp_dist = Net(x_tr.shape[1],2)\n",
    "    optimizer = optim.RMSprop(mlp_dist.parameters(),lr=l_r)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred,_ = mlp_dist(x_tr)\n",
    "        # Compute and print loss\n",
    "        loss1 = (1-l)*criterion(y_pred, y_tr)\n",
    "        loss2 = t*t*l*criterion(y_pred, p_tr)\n",
    "        loss = loss1 + loss2\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    output,_=mlp_dist(x_te)\n",
    "    pred = torch.argmax(output,dim=1)\n",
    "    pred=pred.numpy()\n",
    "    res_dis=np.mean(pred==np.argmax(y_te,1))\n",
    "    \"\"\"\n",
    "    Training a student model using a CDF transformed Z.\n",
    "    \"\"\"\n",
    "    # z_tr = Variable(torch.from_numpy(z_tr*1.0)).type(torch.FloatTensor)\n",
    "    # mlp_z = Net(x_tr.shape[1],2)\n",
    "    # optimizer = optim.RMSprop(mlp_z.parameters(),lr=l_r)\n",
    "    # mlp_z=fitModel(mlp_z,optimizer,criterion,epochs,x_tr,z_tr)\n",
    "    # output,_=mlp_z(x_te)\n",
    "    # pred = torch.argmax(output,dim=1)\n",
    "    # pred=pred.numpy()\n",
    "    # res_cdf=np.mean(pred==np.argmax(y_te,1))\n",
    "\n",
    "    #return np.array([res_priv, res_reg, res_dis, res_cdf])\n",
    "    return np.array([res_priv, res_reg, res_dis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77567440",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c17e644",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_size\tPrivileged\tNo PI\t\tGeneralised Distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [03:14<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\t\t0.95(+/-0.01)\t0.87(+/-0.02)\t0.95(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [05:02<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\t\t0.97(+/-0.02)\t0.85(+/-0.03)\t0.96(+/-0.02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [05:56<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\t\t0.95(+/-0.01)\t0.92(+/-0.01)\t0.95(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [05:28<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\t\t0.97(+/-0.02)\t0.93(+/-0.01)\t0.97(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [07:57<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t\t0.95(+/-0.01)\t0.94(+/-0.01)\t0.95(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [07:26<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t\t0.97(+/-0.02)\t0.95(+/-0.01)\t0.97(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [04:05<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\t\t0.95(+/-0.01)\t0.95(+/-0.01)\t0.95(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [03:54<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\t\t0.98(+/-0.01)\t0.96(+/-0.01)\t0.97(+/-0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# experiment hyper-parameters\n",
    "d      = 50\n",
    "n_tr   = 2000\n",
    "n_te   = 1000\n",
    "n_reps = 100\n",
    "\n",
    "results = {}\n",
    "\n",
    "## Scaling the sample size\n",
    "#print(\"Training_size\\tPrivileged\\tNo PI\\t\\tGeneralised Distillation\\tCDF Student\")\n",
    "print(\"Training_size\\tPrivileged\\tNo PI\\t\\tGeneralised Distillation\")\n",
    "mean_results = {\n",
    "    \"Experiment name\": [],\n",
    "    \"Training size\": [],\n",
    "    \"Privileged\": [],\n",
    "    \"Generalized Distillation\": [],\n",
    "    \"No PI\": [],\n",
    "    \"CDF student\": []\n",
    "}\n",
    "\n",
    "std_results = {\n",
    "    \"Experiment name\": [],\n",
    "    \"Training size\": [],\n",
    "    \"Privileged\": [],\n",
    "    \"Generalized Distillation\": [],\n",
    "    \"No PI\": [],\n",
    "    \"CDF student\": []\n",
    "}\n",
    "\n",
    "for n_tr in [200, 500, 1000, 2000]:\n",
    "    np.random.seed(0)\n",
    "    for experiment in [synthetic_01, synthetic_03]:\n",
    "        R = np.zeros((n_reps,3))\n",
    "        for rep in tqdm(range(n_reps)):\n",
    "            a   = np.random.randn(50)\n",
    "            (xs_tr,x_tr,y_tr) = experiment(a,n=n_tr)\n",
    "            (xs_te,x_te,y_te) = experiment(a,n=n_te)\n",
    "            R[rep,:] += do_exp(x_tr,xs_tr,y_tr,x_te,xs_te,y_te)\n",
    "        means = R.mean(axis=0).round(2)\n",
    "        stds  = R.std(axis=0).round(2)\n",
    "        #print(f\"{n_tr}\\t\\t{means[0]}(+/-{stds[0]})\\t{means[1]}(+/-{stds[1]})\\t{means[2]}(+/-{stds[2]})\\t\\t\\t{means[3]}(+/-{stds[3]})\\t\")\n",
    "        print(f\"{n_tr}\\t\\t{means[0]}(+/-{stds[0]})\\t{means[1]}(+/-{stds[1]})\\t{means[2]}(+/-{stds[2]})\")\n",
    "        \n",
    "        mean_results[\"Experiment name\"].append(experiment.__name__)\n",
    "        mean_results[\"Training size\"].append(n_tr)\n",
    "        mean_results[\"Privileged\"].append(means[0])\n",
    "        mean_results[\"No PI\"].append(means[1])\n",
    "        mean_results[\"Generalized Distillation\"].append(means[2])\n",
    "        #mean_results[\"CDF student\"].append(means[3])\n",
    "        std_results[\"Experiment name\"].append(experiment.__name__)\n",
    "        std_results[\"Training size\"].append(n_tr)\n",
    "        std_results[\"Privileged\"].append(stds[0])\n",
    "        std_results[\"No PI\"].append(stds[1])\n",
    "        std_results[\"Generalized Distillation\"].append(stds[2])\n",
    "        #std_results[\"CDF student\"].append(stds[3])\n",
    "        \n",
    "        results[experiment.__name__] = R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}